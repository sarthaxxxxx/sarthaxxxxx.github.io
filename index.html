<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sarthak Kumar Maharana</title>
  
  <meta name="author" content="Sarthak Kumar Maharana">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><b>Sarthak Kumar Maharana</b></name>
              </p>

              <p>
                I'm a second-year CS PhD student at the University of Texas at Dallas (UTD), advised by <a href = https://yunhuiguo.github.io/> Dr. Yunhui Guo</a>. Before this, I obtained my MS in Electrical Engineering from the University of Southern California (USC) and a Bachelor's degree from IIIT Bhubaneswar (IIIT-Bh), India, with an honors degree in Electrical and Electronics Engineering.              </p>
              
              <p>
                My research interests broadly encompass multimodal learning (Vision + X) with a larger focus on the efficient adaptation of models to distributional shifts, improving their robustness, and generalization at test-time. Additional interests lie in data-centric ML (continual, few-shot, transfer learning), and human-centered AI. I am also engaged in projects on active learning and machine unlearning in generative models.
              </p>

              <p>
                During my Masters, I closely worked with <a href="https://scholar.google.com/citations?user=sm2Y-6sAAAAJ&hl=en"> Dr. Yonggang Shi</a>. Previously, I had also worked with <a href = "https://scholar.google.com/citations?hl=en&user=8EDHmYkAAAAJ&view_op=list_works&sortby=pubdate">Dr. Shri Narayanan</a>. As an undergraduate, I was fortunate enough to work with <a href="https://scholar.google.com/citations?user=rcF7N44AAAAJ&hl=en&oi=ao">Dr. Ren Hongliang</a> (NUS), <a href="https://scholar.google.com/citations?user=B_yn0m0AAAAJ&hl=en"> Dr. Prasanta Kumar Ghosh</a> (IISc), and <a href="https://scholar.google.com/citations?hl=en&user=VYAwEGUAAAAJ"> Dr. Aurobinda Routray</a> (IIT-Kharagpur).
              </p>

	       <p>
                I have published at top-tier ML/computer vision/signal processing conferences such as <b>NeurIPS(2x)</b>, <b>AAAI</b>, <b>ECCV</b>, and <b>ICASSP(2x)</b>. 
              </p>

	       <p>
                <font color = 'red'> I'm happy to chat and discuss potential collaborations. Feel free to contact me.</font>
              </p>

	       <!-- <p>
                <font color = 'green'> I'm constantly trying to push the frontiers of ML research by searching good ways to foster the development of important skills and "adaptively fine-tune and distill knowledge"  (slipped in a nerdy joke here haha). Join my "brand new" <a href="https://discord.gg/SGbjwU6Awv">Discord</a> server! üëâüèºüëàüèª </font>
              </p> -->
		    
              <p style="text-align:center">
                <a href="mailto:sarthakmaharana9811@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Sarthak_CV.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=1sIJMUgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/sarthaxxxxx/">GitHub</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sarthak9811/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:1.5%;width:50%;max-width:50%">
              <a href="images/self_2.jpg"><img style="width:90%;max-width:100%" alt="profile photo" src="images/self_2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				 -->

        </div>
        

      <!-- Education -->
      <!-- <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-education" id="education"><heading>Education</heading></button>
      <div id="content-experience" class="collapse in">
			
        <table border=0 class="bg_colour" style="padding:2px;width:100%;border:2px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>

          <tr>
              <td style="padding:10px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='images/utd.png' width="90" class="side-image">
                  </div>
              </td>
              <td style="padding:1px;width:75%;vertical-align:top">
                   <papertitle style="color:gray"><big>Software Development Engineer Intern</big> </papertitle> <papertitle ><big> | Expedia Group</big></papertitle> -->
                  <!-- <papertitle><big>University of Texas at Dallas</big></papertitle>
                  <br>
                  <papertitle style="color:green"><big>Doctor of Philosophy (PhD), </big></papertitle><papertitle><big>Computer Science</big></papertitle>
                  <br>
                </papertitle><papertitle><big>GPA - </big></papertitle> -->
                <!-- <br>
<i>Dallas, TX</i>
                  <br>                              							    
                  August 2023 - May 2027
              </td>
          </tr> -->
      <!-- </tbody></table> --> 

      <!-- <table border=0 class="bg_colour" style="padding:2px;width:100%;border:0px;border-spacing:2px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>

        <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='images/USC-logo.png' width="90" class="side-image">
                </div>
            </td>
            <td style="padding:1px;width:75%;vertical-align:top">
                <<papertitle style="color:gray"><big>Software Development Engineer Intern</big> </papertitle> <papertitle ><big> | Expedia Group</big></papertitle> -->
                <!-- <papertitle><big>University of Southern California</big></papertitle>
                <br>
                <papertitle style="color:green"><big>Master of Science (MS), </big></papertitle><papertitle><big>Electrical Engineering</big></papertitle>
                <br>
                <papertitle><big>GPA - 3.85/4</big></papertitle>
                <br> -->
<!-- <i>Los Angeles, CA</i>
<br>
                August 2021 - May 2023
            </td>
        </tr>

    </tbody></table>

    <table border=0 class="bg_colour" style="padding:2px;width:100%;border:0px;border-spacing:2px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>

      <tr>
          <td style="padding:10px;width:25%;vertical-align:middle">
              <div class="one">
                  <img src='images/IIIT.png' width="90" class="side-image">
              </div>
          </td>
          <td style="padding:1px;width:75%;vertical-align:top">
              <papertitle style="color:gray"><big>Software Development Engineer Intern</big> </papertitle> <papertitle ><big> | Expedia Group</big></papertitle> -->
              <!-- <papertitle><big>International Institute of Information Technology Bhubaneswar (IIIT-Bh)</big></papertitle>
              <br>
              <papertitle style="color:green"><big>Bachelor of Technology (BTech), </big></papertitle><papertitle><big>Electrical and Electronics Engineering</big></papertitle>
              <br>
              <papertitle><big>GPA - 8.32/10</big></papertitle>
              <br> -->
<!-- <i>Bhubaneswar, India</i>
<br>
              August 2016 - June 2020
          </td> -->
      <!-- </tr> --> 

  <!-- </tbody></table> --> 

</div>


      <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-education" id="education"><heading>News</heading></button>
      <div id="content-experience" class="collapse in">
                <!-- <div class="scroll">  -->

                <table border=0 class="bg_colour" style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

		<tr>
                  <td>
                      <p style="color:green; display:inline">Dec '24 &nbsp</p>
                  </td>
                  <td>
                   <a href="https://arxiv.org/abs/2403.10650v2">PALM</a> has been accepted to AAAI 2025 for an <font color = 'red'>Oral</font> presentation!
                  </td>
                </tr>

		<tr>
                  <td>
                      <p style="color:green; display:inline">Nov '24 &nbsp</p>
                  </td>
                  <td>
                  Serving as a <a href="https://cvpr.thecvf.com/">CVPR 2025</a> reviewer. 
                  </td>
                </tr>
			
		<tr>
                  <td>
                      <p style="color:green; display:inline">Oct '24 &nbsp</p>
                  </td>
                  <td>
                  Variational Diffusion Unlearning (VDU) is accepted to the NeurIPS SafeGenAI workshop 2024!  
                  </td>
                </tr>
			
		<tr>
                  <td>
                      <p style="color:green; display:inline">Sep '24 &nbsp</p>
                  </td>
                  <td>
                  Our paper on submodular optimization for active 3D object detection has been accepted to NeurIPS 2024! 
                  </td>
                </tr>
			
		<tr>
                  <td>
                      <p style="color:green; display:inline">Aug '24 &nbsp</p>
                  </td>
                  <td>
                  Serving as a reviewer for <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a>.
                  </td>
                </tr>
			
		<tr>
                  <td>
                      <p style="color:green; display:inline">Jul '24 &nbsp</p>
                  </td>
                  <td>
                  Our paper on <a href="https://arxiv.org/abs/2403.10663">DNN watermarking</a> has been accepted to ECCV 2024!
                  </td>
                </tr>
			
		<tr>
                  <td>
                      <p style="color:green; display:inline">May '24 &nbsp</p>
                  </td>
                  <td>
                  Serving as a reviewer for <a href="https://bmvc2024.org/">BMVC 2024</a>.
                  </td>
                </tr>
			
		<tr>
                  <td>
                      <p style="color:green; display:inline">Mar '24 &nbsp</p>
                  </td>
                  <td>
                  Serving as a reviewer for <a href="https://tta-cvpr2024.github.io/">CVPR 2024 Workshop on Test-Time Adaptation: Model, Adapt Thyself! (MAT)</a>.
                  </td>
                </tr>
			
		<tr>
                  <td>
                      <p style="color:green; display:inline">Feb '24 &nbsp</p>
                  </td>
                  <td>
                  Serving as a reviewer for <a href="https://eccv.ecva.net/">ECCV 2024</a>.
                  </td>
                </tr>
			
		<tr>
                  <td>
                      <p style="color:green; display:inline">Jan '24 &nbsp</p>
                  </td>
                  <td>
                  Our paper on SSL features for dysarthric speech has been accepted to the SASB workshop @ ICASSP 2024!
                  </td>
                </tr>
			
		<tr>
                    <td>
                        <p style="color:green; display:inline">Jan '24 &nbsp</p>
                    </td>
                    <td>
                    I am glad to be selected to attend the <a href="https://www.oxfordml.school/">MLx Representation Learning and Generative AI Oxford Summer School</a>.
                    </td>
                </tr>
			
                  <!-- <tr>
                    <td>
                        <p style="color:green; display:inline">Aug '23 &nbsp</p>
                    </td>
                    <td>
                    Started PhD @ UTD!
                    </td>
                </tr>

                  <tr>
                    <td>
                        <p style="color:green; display:inline">May '23 &nbsp</p>
                    </td>
                    <td>
                    Graduated from USC with an MS in Electrical Engineering!
                    </td>
                </tr>
      
                  <tr>
                        <td>
                            <p style="color:green; display:inline">Mar '23 &nbsp</p>
                        </td>
                        <td>
				Accepted the CS PhD offer from UTD!
                        </td>
                    </tr>
                  
                  <tr>
                      <td>
                          <p style="color:green; display:inline">Aug '21 &nbsp</p>
                      </td>
                      <td>
                          Started MS in Electrical Engineering at USC!
                      </td>
                  </tr>

                  <tr>
                    <td>
                      <p style="color:green; display:inline">June '21 &nbsp</p>
                    </td>
                    <td>
                      Virtually presented the paper on acoustic-to-articulatory inversion of dysarthric speech at IEEE ICASSP 2021.
                    </td>
                  </tr>
			
			<tr>
                        <td>
                            <p style="color:green; display:inline">Mar '21 &nbsp</p>
                        </td>
                        <td>
			Our paper on <a href="https://ieeexplore.ieee.org/document/9413625">acoustic-to-articulatory inversion using cross-corpus data</a> was accepted to IEEE ICASSP 2021!
                        </td>
                    </tr>	

			
			<tr>
                        <td>
                            <p style="color:green; display:inline">Jun '20 &nbsp</p>
                        </td>
                        <td>
				Graduated from IIIT-Bh with a BTech (Honors) in Electrical and Electronics Engineering.
                        </td>
                    </tr> -->

<!--                     <tr>
                      <td>
                          <p style="color:green; display:inline">Nov '19 &nbsp</p>
                      </td>
                      <td>
      A paper on harmonic analysis of a PV hysteresis current control inverted got accepted to IEEE ICSSIT 2019!
                      </td>
                  </tr> -->


                </tbody>
              </table>
                <!-- </div> -->
                <!-- </div> -->
                <!-- </div> -->
                <!--  -->

               </div>
              
              <div id="content-experience" class="collapse in"> 
              <table border=0 class="bg_colour" style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>
              

              <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-education" id="education"><heading>Publications / Preprints</heading></button>
              <div id="content-experience" class="collapse in">
                  <!-- <table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>
                   <tr>
                    <ul>
                      <li>I'm primarily interested in human-centered AI, which includes multi-modal machine learning with applications to speech and medical images. On the whole, my interests lie in self-supervised learning, video representation learning, generative models, etc.</li>
                  </td>
                  </tr>
                  </tbody>
                </table> -->
                <tr>
                  <td style="padding:1px;width:75%;vertical-align:middle">
                    <p>
                      First author works are <span class="highlight">highlighted</span>.
                    </p>
                  </td>

		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()" bgcolor="#ffffd0">
                      <td style="padding:20px;width:25%;vertical-align:middle">
                       <div class="one">
                          <div class="two" id='neurips_image'>
                              <a href="images/framework_v8.jpg"><img style="width:120%;max-width:120%;height:90%;" alt="" src="images/framework_v8.jpg" class="hoverZoomLink"></a>
                        </div>
                      </div>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="">
                        <papertitle>Enhancing Robustness of CLIP to Common Corruptions through Bimodal Test-Time Adaptation</papertitle>
                        </a>
                        <br>
                        <strong>Sarthak Kumar Maharana</strong>,
                        Baoming Zhang,
                        <a href="https://scholar.google.com/citations?user=WbO7tjYAAAAJ&hl=en">Leonid Karlinsky</a>,
                        <a href="https://www.rogerioferis.org/">Rogerio Feris</a>,
                        <a href="https://yunhuiguo.github.io/">Yunhui Guo</a>
                        <br>
                        <em>Under Review</em><br>
                        <!-- <em>ECCV 2022 Workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications</em> -->
                        <br>
                        <a href="https://arxiv.org/abs/2412.02837">[arXiv]</a> 
                        <br>
                        <p>Bimodal test-time adaptation method designed to improve CLIP's robustness to common image corruptions.</p>
                      </td>
                    </tr>

		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()" bgcolor="#ffffd0">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                     <div class="one">
                        <div class="two" id='neurips_image'>
                            <a href="images/PALM05.png"><img style="width:120%;max-width:120%;height:90%;" alt="" src="images/PALM05.png" class="hoverZoomLink"></a>
                      </div>
                    </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="">
                      <papertitle>PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation</papertitle>
                      </a>
                      <br>
                      <strong>Sarthak Kumar Maharana</strong>,
                      Baoming Zhang,
                      <a href="https://yunhuiguo.github.io/">Yunhui Guo</a>
                      <br>
		      <em>In AAAI 2025</em><br>
                      <!-- <em>IEEE International Conference of Acoustics, Speech, and Signal Processing (ICASSP)</em> 2021 <br> -->
                      <!-- <em>ECCV 2022 Workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications</em> -->
                      <br>
                      <a href="https://arxiv.org/abs/2403.10650">[Paper]</a> 
		      <a href = "https://sarthaxxxxx.github.io/PALM.github.io/">[Project]</a>
		      <a href="https://github.com/sarthaxxxxx/PALM">[Code]</a>
                      <br>
                      <p></p>
                      <p>Adaptive learning rate continual test-time adaptation method based on model prediction uncertainty and parameter sensitivity to rapid distributional shifts. </p>
                    </td>
                  </tr>

		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
                      <td style="padding:20px;width:25%;vertical-align:middle">
                       <div class="one">
                          <div class="two" id='neurips_image'>
                              <a href="images/VDU.drawio.png"><img style="width:110%;max-width:150%;height:80%;" alt="" src="images/VDU.drawio.png" class="hoverZoomLink"></a>
                        </div>
                      </div>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="">
                        <papertitle>Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models</papertitle>
                        </a>
                        <br>
                        <a href="https://subhodip123.github.io/">Subhodip Panda</a>,
                        MS Varun, Shreyans Jain,
                        <strong>Sarthak Kumar Maharana</strong>,
                        <a href="https://sites.google.com/view/prathosh/home">Prathosh AP</a>
                        <br>
                        <em>In NeurIPS Safe Generative AI Workshop 2024</em> <br>
                        <!-- <em>ECCV 2022 Workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications</em> -->
                        <br>
                        <a href="https://openreview.net/forum?id=B2wDjiED9V">[Paper]</a> 
                        <br>
                        <p>
                          Machine unlearning of user-specific classes/concepts in pre-trained diffusion models (DDPMs).
                        </p>
                        <!-- <p>With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as multi-view data, it becomes feasible to effectively defend functionality stealing attacks. Based on this perspective, we introduce a novel watermarking technique based on Multi-view dATa, called MAT, for efficiently embedding watermarks within DNNs. This approach involves constructing a trigger set with multi-view data and incorporating a simple feature-based regularization method for training the source model. We validate our method across various benchmarks and demonstrate its efficacy in defending against model extraction attacks, surpassing relevant baselines by a significant margin. </p> -->
                      </td>
                    </tr>

                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
                      <td style="padding:20px;width:25%;vertical-align:middle">
                       <div class="one">
                          <div class="two" id='neurips_image'>
                              <a href="images/stone.png"><img style="width:110%;max-width:150%;height:80%;" alt="" src="images/stone.png" class="hoverZoomLink"></a>
                        </div>
                      </div>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="">
                        <papertitle>STONE: A Submodular Optimization Framework for Active 3D Object Detection</papertitle>
                        </a>
                        <br>
                        Ruiyu Mao,
                        <strong>Sarthak Kumar Maharana</strong>,
                        <a href="https://sites.google.com/view/rishabhiyer/">Rishabh K Iyer</a>,
                        <a href="https://yunhuiguo.github.io/">Yunhui Guo</a>
                        <br>
                        <em>In NeurIPS 2024</em><br>
                        <!-- <em>ECCV 2022 Workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications</em> -->
                        <br>
                        <a href="https://arxiv.org/abs/2410.03918">[Paper]</a> 
                        <a href="https://github.com/RuiyuM/STONE">[Code]</a>
                        <br>
                        <p>
                          A submodular optimization scheme to handle data imbalance and label distributional coverage for active 3D object detection.
                        </p>
                        <!-- <p>With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as multi-view data, it becomes feasible to effectively defend functionality stealing attacks. Based on this perspective, we introduce a novel watermarking technique based on Multi-view dATa, called MAT, for efficiently embedding watermarks within DNNs. This approach involves constructing a trigger set with multi-view data and incorporating a simple feature-based regularization method for training the source model. We validate our method across various benchmarks and demonstrate its efficacy in defending against model extraction attacks, surpassing relevant baselines by a significant margin. </p> -->
                      </td>
                    </tr>


                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
                      <td style="padding:20px;width:25%;vertical-align:middle">
                       <div class="one">
                          <div class="two" id='neurips_image'>
                              <a href="images/pipeline.png"><img style="width:110%;max-width:110%;height:75%;" alt="" src="images/pipeline.png" class="hoverZoomLink"></a>
                        </div>
                      </div>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="">
                        <papertitle>Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data</papertitle>
                        </a>
                        <br>
                        Yuxuan Li,
                        <strong>Sarthak Kumar Maharana</strong>,
                        <a href="https://yunhuiguo.github.io/">Yunhui Guo</a>
                        <br>
                        <em>In ECCV 2024</em><br>
                        <!-- <em>ECCV 2022 Workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications</em> -->
                        <br>
                        <a href="https://arxiv.org/abs/2403.10663">[Paper]</a> 
                        <a href="https://github.com/liyuxuan-github/MAT">[Code]</a>
                        <br>
                        <p>
                          Novel watermarking technique based on multi-view data for defending against model extraction attacks.
                        </p>
                        <!-- <p>With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as multi-view data, it becomes feasible to effectively defend functionality stealing attacks. Based on this perspective, we introduce a novel watermarking technique based on Multi-view dATa, called MAT, for efficiently embedding watermarks within DNNs. This approach involves constructing a trigger set with multi-view data and incorporating a simple feature-based regularization method for training the source model. We validate our method across various benchmarks and demonstrate its efficacy in defending against model extraction attacks, surpassing relevant baselines by a significant margin. </p> -->
                      </td>
                    </tr>

                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()" bgcolor="#ffffd0">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                     <div class="one">
                        <div class="two" id='neurips_image'>
                            <a href="images/tSNE_each_sub1.png"><img style="width:110%;max-width:130%;height:85%;" alt="" src="images/tSNE_each_sub1.png" class="hoverZoomLink"></a>
                      </div>
                    </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="">
                      <papertitle>Acoustic-to-Articulatory Inversion for Dysarthric Speech: Are Pre-Trained Self-Supervised Representations Favorable?
                      </papertitle>
                      </a>
                      <br>
                      <strong>Sarthak Kumar Maharana</strong>,
                      <a href="https://www.linkedin.com/in/krishna-kamal">Krishna Kamal Adidam</a>,
                      <a href='https://www.linkedin.com/in/shoumik-nandi-766b82175'>Shoumik Nandi</a>,
                      <a href="https://www.ajitesh-srivastava.com/">Ajitesh Srivastava</a>
                      <br>
                      <em>In ICASSP 2024 Workshop on Self-supervision in Audio, Speech, and Beyond (SASB) 2024</em><br>
                      <!-- <em>ECCV 2022 Workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications</em> -->
                      <br>
                      <a href="https://arxiv.org/abs/2309.01108">[Paper]</a>
		      <a href="https://drive.google.com/file/d/1TpzYqzfHYSTpwdl0qwG2sZdMtFKhQQj0/view?usp=drive_link">[Poster]</a>
                      <br>
                      <p>
                        Effectiveness of pre-trained self-supervised learning representations for acoustic-to-articulatory inversion of dysarthric speech.
                      </p>
                      <!-- <p>Acoustic-to-articulatory inversion (AAI) involves mapping from the
                        acoustic to the articulatory space. Signal-processing features like
                        the MFCCs, have been widely used for the AAI task. For subjects
                        with dysarthric speech, AAI is challenging because of an imprecise and indistinct pronunciation. In this work, we perform AAI
                        for dysarthric speech using representations from pre-trained selfsupervised learning (SSL) models. We demonstrate the impact of
                        different pre-trained features on this challenging AAI task, at low-resource conditions. In addition, we also condition x-vectors to the
                        extracted SSL features to train a BLSTM network. In the seen case,
                        we experiment with three AAI training schemes (subject-specific,
                        pooled, and fine-tuned).</p> -->
                    </td>
                  </tr>

                   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()" bgcolor="#ffffd0">
                      <td style="padding:20px;width:25%;vertical-align:middle">
                       <div class="one">
                          <div class="two" id='neurips_image'>
                              <a href="images/icassp.png"><img style="width:110%;max-width:110%;height:75%;" alt="" src="images/icassp.png" class="hoverZoomLink"></a>
                        </div>
                      </div>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="">
                        <papertitle>Acoustic-to-Articulatory Inversion for Dysarthric Speech by Using Cross-Corpus Acoustic-Articulatory Data</papertitle>
                        </a>
                        <br>
                        <strong>Sarthak Kumar Maharana</strong>,
                        <a href="https://in.linkedin.com/in/aravind-illa-13a84658">Aravind Illa</a>,
                        <a href='https://www.linkedin.com/in/renuka-mannem-17686617a'>Renuka Mannem</a>,
                        Yamini Bellur, Veeramani Preethish Kumar, Seena Vengalil, Kiran Polavarapu, Nalini Atchayaram,
                        <a href="https://scholar.google.com/citations?hl=en&user=B_yn0m0AAAAJ&view_op=list_works">Prasanta Kumar Ghosh</a>
                        <br>
                        <em>In ICASSP 2021</em><br>
                        <!-- <em>ECCV 2022 Workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications</em> -->
                        <br>
                        <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:qks0yUjnpGsJ:scholar.google.com/&output=citation&scisdr=ClGT-4YPEMLP2VuiMZQ:AFWwaeYAAAAAZPqkKZS7Nl0vfUdvI_i5NHipe2c&scisig=AFWwaeYAAAAAZPqkKdQi3i0lt4hyyM-OJYM07LA&scisf=4&ct=citation&cd=-1&hl=en">[BibTeX]</a>
                        <a href="https://ieeexplore.ieee.org/document/9413625">[Paper]</a> 
                        <a href="https://github.com/sarthaxxxxx/AAI-ALS">[Code]</a> 
                        <a href="https://drive.google.com/file/d/1Oj19XvG26dIKCfwjDhqt0rrEnfbVrwFK/view?usp=sharing">[Video]</a>
                        <br>
                        <p>
                          Joint and multi-corpus training for acoustic-to-articulatory inversion of dysarthric speech, using x-vectors, at low-resource data conditions.
                        </p>
                        <!-- <p>In this work, we focus on estimating articulatory movements from acoustic features, known as acoustic-to-articulatory inversion (AAI), for dysarthric patients with amyotrophic lateral sclerosis (ALS). Unlike healthy subjects, there are two potential challenges involved in AAI on dysarthric speech. Due to speech impairment, the pronunciation of dysarthric patients is unclear and inaccurate, which could impact the AAI performance. In addition, acoustic-articulatory data from dysarthric patients is limited due to the difficulty in recording. These challenges motivate us to utilize cross-corpus acoustic-articulatory data. In this study, we propose an AAI model by conditioning speaker information using x-vectors at the input, and multi-target articulatory trajectory outputs for each corpus separately. </p> -->
                      </td>
                    </tr>


                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
                        <td style="padding:20px;width:25%;vertical-align:middle">
                         <div class="one">
                            <div class="two" id='neurips_image'>
                                <a href="images/powergui.png"><img style="width:110%;max-width:110%;height:90%;" alt="" src="images/powergui.png" class="hoverZoomLink"></a>
                          </div>
                        </div>
                        </td>
                        <td style="padding:20px;width:90%;vertical-align:middle">
                          <a href="">
                          <papertitle>Harmonics analysis of a PV integrated hysteresis current control inverter connected with grid and without grid</papertitle>
                          </a>
                          <br>
                          Jayanta Kumar Sahu, Sudhakar Sahu, J.P Patra,
                          <strong>Sarthak Kumar Maharana</strong>, Bhagabat Panda
                          <br>
                          <em>In ICSSIT 2019</em><br>
                          <!-- <em>ECCV 2022 Workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications</em> -->
                          <br>
                          <a href = "https://scholar.googleusercontent.com/scholar.bib?q=info:DOqHloR39YQJ:scholar.google.com/&output=citation&scisdr=ClGT-4YPEMLP2Vulqxw:AFWwaeYAAAAAZPqjsxwWTw5uzNsKkYxdbxISbYE&scisig=AFWwaeYAAAAAZPqjsyr7R9lqiNlL3vb_zKrtG4I&scisf=4&ct=citation&cd=-1&hl=en">[BibTeX]</a>
                          <a href="https://ieeexplore.ieee.org/abstract/document/8987864">[Paper]</a> 
                          <!-- <a href="https://github.com/sarthaxxxxx/AAI-ALS">Code</a> / -->
                          <!-- <a href="https://drive.google.com/file/d/1hukbcGmI6zk4Rrq9H-tE35YoC5V6dbjs/view?usp=sharing">Video</a> -->
                          <br>
                          <p>
                            Harmonics analysis of a PV integrated hysteresis current control inverter connected with grid and without grid.
                          </p>
                          <!-- <p>Generally, two devices are responsible for the generation of time-variant power. They are alternators and inverters. Harmonics are the unwanted signals generally created on the output of the inverter. In this paper, hysteresis current control inverters are described. Here the HCC inverters are connected with a grid and without a grid and integrated with a photo voltaic panel. The HCC inverters are connected to the grid with the help of a phase lock loop. Finally, the total harmonics distortion is calculated in this model their results are compared based on total harmonics distortion. </p> -->
                        </td>
                      </tr>
                      </tbody></table>


                    </div>
                

          <!-- <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-education" id="education"><heading>Awards</heading></button>
          <div id="content-experience" class="collapse in">
          <table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>
            <tr>
              <ul>
                <li>Fully funded tuition, with a stipend, to pursue a CS PhD at UTD.</li>
                <li>Governing Body Merit Scholarship (April 2021).</li>
                <font color="grey" style="italic" size="2pt"><em>&nbsp&nbsp Awarded to top 3 students of each department at IIIT-Bh. 
                  Received for the academic year 2019-2020.</em></font>
                <li>Indian Academy of Sciences (IAS) - Summer Research Fellowship (April 2019)</li>
                <font color="grey" style="italic" size="2pt"><em>&nbsp&nbsp An annual research fellowship program (<10% selection rate) conducted by the Indian Academy of Sciences, under IISc Bangalore.</em></font>
            </td>
            </tr>
            </tbody>
          </table>
         -->
        

          <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-education" id="education"><heading>Academic/Volunteer Work</heading></button>
          <div id="content-experience" class="collapse in">
          <table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>
            <tr>
              <ul>
		<li>Reviewer - CVPR 2025, ICLR 2025, NeurIPS Workshops 2024, BMVC 2024, ECCV 2024, CVPR Workshops 2024, AAAI 2024 </li>
                <li>Building <a href="https://cordai.org/">CORD.ai</a>, a deep learning research community, as a core member and volunteer researcher. </li>
                <!-- <li>Course Mentor/Grader for graduate level EE 541: An Introduction to Deep Learning (Spring 2022).</li>
                <li>USC IEEE Graduate Society - Member, strengthen academic and social growth of the members, and host workshops.</li>
                <li>PyCon India 2020 - Content writer for social media handles, helped the promotions team to reach out to organizations and colleges, and interacted
                  with individuals who have contributed to the language, and also worked on creating virtual swags.</li> -->
            </td>
            </tr>
            </tbody>
          </table>

          <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-education" id="education"><heading>Miscellaneous</heading></button>
          <div id="content-experience" class="collapse in">
          <table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto"><tbody>
            <tr>
              <ul>
                <li>I'm a cis male.</li>
                <li>I consider myself lucky to have grown up in two beautiful cities in India - Bangalore and Bhubaneswar, that have infused in me a lot of character and development. I've also spent two quality years in the vibrant, diverse, gently warm, and sprawling city of Los Angeles, California. Absolutely look forward to staying in new places and experiencing different cultures.</li>
                <li>I'm a HUGE fan of the classical formats of cricket. You'd often find me watching old test match highlights or SRT straight drives. Nothing can get more sublime than that. I bet! I don't consider IPL/T20 cricket as a thing AT ALL.</li>
                <li>I think mobile photography is like a side gig for me? My phone instantly comes out the moment my eyes catch sight of a beautiful view.</li>
                <li>I also spend a lot of time in quality humor - dark humor per se. We could talk about that later.</li>
            </td>
            </tr>
            </tbody>
          </table>

          
          

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:1px">
                <br>
                <p style="text-align:right;font-size:small;">
                   <a href="https://github.com/jonbarron/jonbarron_website">Source code</a> by Jon Barron.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
                  

                    
</body>

</html>


